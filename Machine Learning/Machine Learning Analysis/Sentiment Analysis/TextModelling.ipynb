{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "criticism_df = pd.read_csv('criticism_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n",
    "    \n",
    "    # 2. Tokenize words\n",
    "    words = word_tokenize(review_text)\n",
    "    \n",
    "    # 3. Convert to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # 4. Remove non-alphabetic characters and numbers\n",
    "    words = [re.sub(\"[^a-zA-Z]\", \"\", word) for word in words]\n",
    "    \n",
    "    # 5. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 6. Create set of stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # 7. Remove stop words\n",
    "    meaningful_words = [word for word in words if word not in stops]\n",
    "    \n",
    "    # 8. Join the words back into one string separated by space\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fachr\\AppData\\Local\\Temp\\ipykernel_47412\\843033685.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(raw_review, 'lxml').get_text()\n"
     ]
    }
   ],
   "source": [
    "preprocessed_criticism_df = criticism_df.copy()\n",
    "preprocessed_criticism_df['english_review'] = preprocessed_criticism_df['english_review'].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_criticism_df['english_review']\n",
    "y = preprocessed_criticism_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data menjadi train dan sementara untuk validation+test (20% dari data)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2024)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the splitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = criticism_df[['place_id','english_review']]\n",
    "# y = criticism_df['label']\n",
    "\n",
    "# # Split data menjadi train dan sementara untuk validation+test (20% dari data)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=2024)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=2024)\n",
    "\n",
    "# # Combine train and validation data\n",
    "# X_train_val = pd.concat([X_train, X_val])\n",
    "# y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "# # Combine train, validation, and test data\n",
    "# X_all = pd.concat([X_train, X_val, X_test])\n",
    "# y_all = pd.concat([y_train, y_val, y_test])\n",
    "\n",
    "# # Create DataFrames\n",
    "# train_val_df = pd.concat([X_train_val, y_train_val], axis=1)\n",
    "# all_df = pd.concat([X_all, y_all], axis=1)\n",
    "\n",
    "# # Save DataFrames to CSV\n",
    "# train_val_df.to_csv('train_validation.csv', index=False)\n",
    "# all_df.to_csv('train_validation_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model BERT\n",
    "tfhub_handle_preprocess = \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\"\n",
    "tfhub_handle_encoder = \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-2-h-128-a-2/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)  # Sigmoid untuk binary classification\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(optimizer='adam',\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback for saving the best model based on validation loss\n",
    "checkpoint_filepath = 'FeedbackClassifier.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "249/249 [==============================] - 33s 119ms/step - loss: 0.4864 - accuracy: 0.7706 - val_loss: 0.3981 - val_accuracy: 0.8219\n",
      "Epoch 2/5\n",
      "249/249 [==============================] - 29s 115ms/step - loss: 0.3591 - accuracy: 0.8476 - val_loss: 0.4369 - val_accuracy: 0.8270\n",
      "Epoch 3/5\n",
      "249/249 [==============================] - 29s 118ms/step - loss: 0.3178 - accuracy: 0.8725 - val_loss: 0.4246 - val_accuracy: 0.8441\n",
      "Epoch 4/5\n",
      "249/249 [==============================] - 30s 119ms/step - loss: 0.2686 - accuracy: 0.8988 - val_loss: 0.4502 - val_accuracy: 0.8249\n",
      "Epoch 5/5\n",
      "249/249 [==============================] - 30s 120ms/step - loss: 0.2416 - accuracy: 0.9050 - val_loss: 0.5074 - val_accuracy: 0.8320\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = classifier_model.fit(X_train, y_train,\n",
    "                               validation_data=(X_val, y_val),\n",
    "                               epochs=5,\n",
    "                               batch_size=32,\n",
    "                               callbacks=[model_checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
